
def apply_action(  action_id):
    """Applies preprocessing operation using DataOperations."""
    # Use DataOperations for all preprocessing
    if action_id == 'drop_na':
         data =  data.dropna()
    elif action_id == 'drop_duplicates':
         data =  data.drop_duplicates()
    elif action_id == 'drop_constant':
        constant_cols = [col for col in  data.columns if  data[col].nunique() == 1]
        data =  operations.drop_columns( data, constant_cols)
    elif action_id == 'drop_high_na':
        thresh = len( data) * 0.5
        high_na_cols = [col for col in  data.columns if col !=  target_col and  data[col].isna().sum() > thresh]
        data =  operations.drop_columns( data, high_na_cols)
    elif action_id == 'fill_na':
        for col in  data.columns:
            if  data[col].dtype in ['float64', 'int64']:
                 data[col] =  data[col].fillna( data[col].median())
            else:
                mode_values =  data[col].mode()
                if len(mode_values) > 0:
                     data[col] =  data[col].fillna(mode_values[0])
                else:
                     data[col] =  data[col].fillna('unknown')
    elif action_id == 'encode_categorical':
        # One-hot encode all object columns except target
        cols = [col for col in  data.select_dtypes(include='object').columns if col !=  target_col]
        data =  operations.encode( data, columns=cols)
    elif action_id == 'scale_numeric':
        num_cols = [col for col in  data.select_dtypes(include='number').columns if col !=  target_col]
        data =  operations.scale( data, columns=num_cols, method='standard')
    elif action_id == 'feature_gen':
         data =  operations.generate_features( data)
    # After preprocessing, drop high cardinality columns and non-numeric columns
    if hasattr(  'target_col') and  target_col in  data.columns:
        high_card_cols = [col for col in  data.columns if  data[col].nunique() > 50 and col !=  target_col]
        data =  operations.drop_columns( data, high_card_cols)
        non_numeric_cols = [col for col in  data.columns if col !=  target_col and not pd.api.types.is_numeric_dtype( data[col])]
        data =  operations.drop_columns( data, non_numeric_cols)
        # Train/test split
        X =  data.drop(columns=[ target_col])
        y =  data[ target_col]
        X_train,  X_test,  y_train,  y_test = train_test_split(X, y, test_size=0.2, random_state=42)

def set_model(  model_name, hyperparams):
    """Instantiate and set the model with user-selected hyperparameters."""
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.linear_model import LogisticRegression
    # Add more models as needed
    if model_name.lower() == 'randomforest':
         model_instance = RandomForestClassifier(**hyperparams)
    elif model_name.lower() == 'logisticregression':
         model_instance = LogisticRegression(**hyperparams)
    else:
        print(f"Model {model_name} not recognized. Using LogisticRegression by default.")
        model_instance = LogisticRegression(**hyperparams)

def train(self):
    """Trains ML model."""
    if  model_instance is not None and  X_train is not None and  y_train is not None:
         model_instance.fit( X_train,  y_train)

def evaluate(self):
    """
    Evaluate the trained model with comprehensive metrics.
    
    Returns:
        dict: Evaluation metrics
    """
    if  model_instance is None:
        raise ValueError("No model trained yet. Call train() first.")
    
    if  X_test is None or  y_test is None:
        raise ValueError("No test data available. Ensure data is loaded and split.")
    
    # Make predictions
    y_pred =  model_instance.predict( X_test)
    y_pred_proba = None
    
    # Get prediction probabilities if available
    if hasattr( model_instance, 'predict_proba'):
        try:
            y_pred_proba =  model_instance.predict_proba( X_test)
        except:
            pass
    
    # Calculate comprehensive metrics
    metrics =  evaluator.evaluate( y_test, y_pred, y_pred_proba)
    
    return metrics

def cross_validate(  cv_folds=5):
    """
    Perform cross-validation with comprehensive metrics.
    
    Args:
        cv_folds: Number of cross-validation folds
        
    Returns:
        dict: Cross-validation metrics
    """
    if  model_instance is None:
        raise ValueError("No model trained yet. Call train() first.")
    
    if  X_train is None or  y_train is None:
        raise ValueError("No training data available. Ensure data is loaded and split.")
    
    from sklearn.model_selection import cross_validate, StratifiedKFold
    import numpy as np
    
    # Set up cross-validation
    cv_strategy = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
    
    # Define scoring metrics
    scoring = ['accuracy', 'precision', 'recall', 'f1']
    
    # Perform cross-validation
    cv_results = cross_validate(
         model_instance,  X_train,  y_train, 
        cv=cv_strategy, scoring=scoring, 
        return_train_score=True
    )
    
    # Calculate comprehensive CV metrics
    cv_metrics = {
        'cv_folds': cv_folds,
        'mean_accuracy': float(np.mean(cv_results['test_accuracy'])),
        'std_accuracy': float(np.std(cv_results['test_accuracy'])),
        'mean_precision': float(np.mean(cv_results['test_precision'])),
        'std_precision': float(np.std(cv_results['test_precision'])),
        'mean_recall': float(np.mean(cv_results['test_recall'])),
        'std_recall': float(np.std(cv_results['test_recall'])),
        'mean_f1': float(np.mean(cv_results['test_f1'])),
        'std_f1': float(np.std(cv_results['test_f1'])),
        'train_accuracy': float(np.mean(cv_results['train_accuracy'])),
        'train_std': float(np.std(cv_results['train_accuracy'])),
        'overfitting_score': float(np.mean(cv_results['train_accuracy']) - np.mean(cv_results['test_accuracy'])),
        'stability_score': float(1 - (np.std(cv_results['test_accuracy']) / np.mean(cv_results['test_accuracy']))),
        'all_scores': {
            'test_accuracy': cv_results['test_accuracy'].tolist(),
            'test_precision': cv_results['test_precision'].tolist(),
            'test_recall': cv_results['test_recall'].tolist(),
            'test_f1': cv_results['test_f1'].tolist()
        }
    }
    
    return cv_metrics

def test(self):
    """Tests ML model (could be same as evaluate or extended)."""
    return  evaluate()

def save_model(  path):
    """Saves trained model."""
    if  model_instance is not None:
         registry.save_model( model_instance, 'model', 1)  # Versioning can be improved

def run_full_cycle(  controller, metric_goal=None):
    """
    Run full cyclic workflow: preprocessing, training, testing until metrics are satisfied.
    """
    print("Starting preprocessing cycle...")
    controller.run_preprocessing_cycle(  metric_goal=metric_goal)
    print("Starting training cycle...")
    controller.run_training_cycle(  metric_goal=metric_goal)
    print("Starting testing cycle...")
    controller.run_testing_cycle(  metric_goal=metric_goal)



    def report(self):
        """Generates AI-powered dynamic analysis reports."""
        analysis_results = self.analyzer.analyze(self.data)
        
        # Use AI-powered report generator for intelligent insights
        try:
            from .cycle.reporter import ReportGenerator
            reporter = ReportGenerator(api_key=self.api_key)
            
            # Get model performance if available
            model_performance = None
            if self.model_instance is not None and self.X_test is not None:
                try:
                    model_performance = self.evaluate()
                except:
                    pass
            
            # Generate AI-powered report
            ai_report = reporter.generate_report(analysis_results, model_performance)
            return ai_report
            
        except Exception as e:
            # Fallback to basic analysis with a warning
            print(f"‚ö†Ô∏è  AI report generation failed: {e}")
            print("üìä Falling back to basic analysis...")
            return analysis_results

    def suggestions(self, user_query=None, metrics=None, interactive=True, report=None):
        """
        Advanced AI-powered suggestions with chain-of-thought reasoning and engaging interaction.
        
        Args:
            user_query: User's goal or question
            metrics: Current performance metrics
            interactive: Whether to provide interactive, engaging output
        """
        print("ü§ñ **AI DATA SCIENTIST ANALYSIS** ü§ñ")
        print("=" * 50)
        
        # Get comprehensive analysis
        if report is not None:
            analyzer_result = report
        analyzer_result = self.report()
        
        # Handle case where report() returns a string (fallback mode)
        if analyzer_result is None:
            print("‚ö†Ô∏è  Running in basic mode due to AI initialization issues.")
            print("üìä Using raw data analysis for suggestions...")
            # Convert back to basic analysis for suggester
            analyzer_result = self.analyzer.analyze(self.data)   
        suggestion_response = self.suggester.suggest_next_action(
            analyzer_result, 
            user_query, 
            metrics,
            csv_data=self.data
        )
        
        if interactive:
            self._display_engaging_suggestions(suggestion_response)
        
        return suggestion_response

    def _display_engaging_suggestions(self, response: dict):
        """Display AI suggestions in an engaging, user-friendly format."""
        
        # Display comprehensive CSV analysis first
        csv_analysis = response.get('csv_analysis', {})
        if csv_analysis and isinstance(csv_analysis, dict):
            print(f"\nüìä **COMPREHENSIVE DATA ANALYSIS:**")
            print(f"üìà **Data Quality Score:** {csv_analysis.get('data_quality_score', 'N/A')}/10")
            print(f"üéØ **Modeling Readiness:** {csv_analysis.get('modeling_readiness', 'N/A')}/10")
            
            if 'critical_issues' in csv_analysis and isinstance(csv_analysis['critical_issues'], list) and len(csv_analysis['critical_issues']) > 0:
                print(f"\n‚ö†Ô∏è  **CRITICAL ISSUES IDENTIFIED:**")
                for issue in csv_analysis['critical_issues']:
                    print(f"   ‚Ä¢ {issue}")
            
            if 'key_insights' in csv_analysis and isinstance(csv_analysis['key_insights'], list) and len(csv_analysis['key_insights']) > 0:
                print(f"\nüîç **KEY DATA INSIGHTS:**")
                for insight in csv_analysis['key_insights']:
                    print(f"   ‚Ä¢ {insight}")
            
            if 'preprocessing_recommendations' in csv_analysis and isinstance(csv_analysis['preprocessing_recommendations'], list) and len(csv_analysis['preprocessing_recommendations']) > 0:
                print(f"\nüõ†Ô∏è  **RECOMMENDED PREPROCESSING:**")
                for rec in csv_analysis['preprocessing_recommendations']:
                    print(f"   ‚Ä¢ {rec}")

        # Display context summary
        context_summary = response.get('context_summary', '')
        if context_summary:
            print(f"\nüß† **CONTEXTUAL ANALYSIS:**")
            print(f"{context_summary}")
        
        print(f"\nüß† **STAGE:** {response.get('stage', 'unknown').replace('_', ' ').title()}")
        print(f"üìä **CONFIDENCE:** {response.get('confidence_score', 0):.1%}")
        print("\n" + "="*60)
        
        # Main reasoning with emoji highlights
        reasoning = response.get('reasoning', 'Analysis available - see details above')
        print(f"\nü§î **MY ANALYSIS:**")
        print(f"{reasoning}")
        
        if 'primary_recommendation' in response:
            print(f"\nüéØ **PRIMARY RECOMMENDATION:**")
            print(f"‚úÖ {response['primary_recommendation']}")
        
        if response.get('alternative_options'):
            print(f"\nüîÑ **ALTERNATIVE APPROACHES:**")
            for i, alt in enumerate(response['alternative_options'], 1):
                print(f"   {i}. üí≠ {alt}")
        
        if 'implementation_steps' in response:
            print(f"\nüìã **IMPLEMENTATION STEPS:**")
            for i, step in enumerate(response['implementation_steps'], 1):
                print(f"   {i}. ‚ñ∂Ô∏è {step}")
        
        if 'expected_outcomes' in response:
            print(f"\nüöÄ **EXPECTED OUTCOMES:**")
            print(f"üéâ {response['expected_outcomes']}")
        
        if 'engagement_message' in response:
            print(f"\nüí¨ **ENGAGEMENT MESSAGE:**")
            print(f"{response['engagement_message']}")
        
        if 'next_actions' in response:
            print(f"\n‚è≠Ô∏è  **NEXT POSSIBLE ACTIONS:**")
            for i, action in enumerate(response['next_actions'], 1):
                print(f"   {i}. üéØ {action}")
        
        print("\n" + "="*60)
        print("üöÄ Ready to take action? Choose your next step!")
    
    
    def api(self, api_key: str = None):
        """Initializes LangChain LLM + memory."""
        if api_key:
            self.llm = LLMConnector(api_key=api_key)
        self.memory = []
        # Initialize controller with API key for enhanced workflows
        from data_science_pro.cycle.controller import IntelligentController
        self.controller = IntelligentController()

    def input_data(self, file_path, target_col):
        """Loads dataset via DataLoader."""
        loader = DataLoader()
        self.data = loader.load(file_path)
        self.target_col = target_col
        X = self.data.drop(columns=[self.target_col])
        y = self.data[self.target_col]
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)
